@incollection{sciontiDistributedHPCResources2022,
  title = {Distributed {{HPC Resources Orchestration}} for {{Supporting Large-Scale Workflow Execution}}},
  shorttitle = {{{HPC}}, {{Big Data}}, and {{AI Convergence Towards Exascale}}},
  booktitle = {{{HPC}}, {{Big Data}}, and {{AI Convergence Towards Exascale}}: {{Challenge}} and {{Vision}}},
  author = {Scionti, Alberto and Viviani, Paolo and Vitali, Giacomo and Vercellino, Chiara and Terzo, Olivier and Hachinger, Stephan and Vojacek, Luk{\'a}{\v s}},
  year = {2022},
  month = jan,
  edition = {First},
  pages = {23},
  publisher = {{CRC Press}},
  address = {{New York}},
  doi = {10.1201/9781003176664},
  abstract = {Artificial intelligence (AI) is gaining momentum in the scientific and industrial community for the ever-growing number of applications where such innovative techniques of learning form and processing large amount of data have proved successful. High-performance computing (HPC) and cloud resources providers are moving faster to be able to support new applications that benefit from the combination of traditional HPC simulation, machine learning and deep learning processing and big data analytics. However, the tighter the combination of these three elements is, the more complex the integration of innovative architectures into a single execution platform becomes. On one hand, application workflow management systems need to incorporate more functionalities and support dynamism in the execution, by preserving (energy) efficiency of the infrastructural resources. On the other hand, more exotic hardware accelerators (ranging from GPUs and FPGAs, to neural network processors (NNPs), to neuromorphic processors) need to be integrated in the computing assets in order to leverage performance boost. This chapter provides an overview of the future HPC, AI, and big-data cross-stack execution platform, as devised in the funded EuroHPC ACROSS project, which will be tailored to cope with all these challenges, and to support future exascale-ready applications.},
  isbn = {978-1-00-317666-4},
  langid = {english}
}
@incollection{sciontiEnablingHPCArtificial2022,
  title = {Enabling the {{HPC}} and {{Artificial Intelligence Cross-Stack Convergence}} at the {{Exascale Level}}},
  shorttitle = {{{HPC}}, {{Big Data}}, and {{AI Convergence Towards Exascale}}},
  booktitle = {{{HPC}}, {{Big Data}}, and {{AI Convergence Towards Exascale}}: {{Challenge}} and {{Vision}}},
  author = {Scionti, Alberto and Viviani, Paolo and Vitali, Giacomo and Vercellino, Chiara and Terzo, Olivier},
  year = {2022},
  month = jan,
  edition = {First},
  pages = {22},
  publisher = {{CRC Press}},
  address = {{New York}},
  doi = {10.1201/9781003176664},
  abstract = {Artificial intelligence (AI) is gaining momentum in the scientific and industrial community for the ever-growing number of applications where such innovative techniques of learning form and processing large amount of data have proved successful. High-performance computing (HPC) and cloud resources providers are moving faster to be able to support new applications that benefit from the combination of traditional HPC simulation, machine learning and deep learning processing and big data analytics. However, the tighter the combination of these three elements is, the more complex the integration of innovative architectures into a single execution platform becomes. On one hand, application workflow management systems need to incorporate more functionalities and support dynamism in the execution, by preserving (energy) efficiency of the infrastructural resources. On the other hand, more exotic hardware accelerators (ranging from GPUs and FPGAs, to neural network processors (NNPs), to neuromorphic processors) need to be integrated in the computing assets in order to leverage performance boost. This chapter provides an overview of the future HPC, AI, and big-data cross-stack execution platform, as devised in the funded EuroHPC ACROSS project, which will be tailored to cope with all these challenges, and to support future exascale-ready applications.},
  isbn = {978-1-00-317666-4},
  langid = {english}
}
@inproceedings{21:lexis,
	title        = {HPC-Cloud-Big Data convergent architectures and research data management: The LEXIS approach},
	author       = {Hachinger S. and Martinovič J. and Terzo O. and Levrier M. and Scionti A. and Magarielli D. and Goubier T. and Parodi A. and Harsh P. and Apopei F.I. and Munke J. and García-Hernández R.J. and Golasowski M. and Hayek M. and Donnat F. and Ganne L. and Koch-Hofer C. and Vitali G. and Viviani P. and Schorlemmer D.},
	year         = {2021},
	journal      = {PoS: Proceedings of Science},
	volume       = 378,
	issn         = {1824-8039},
	abbrev_source_title = {Pos proc. sci.},
	abstract     = {The LEXIS project (Large-scale EXecution for Industry & Society, H2020 GA825532) provides a platform for optimised execution of Cloud-HPC workflows, reducing computation time and increasing energy efficiency. The system will rely on advanced, distributed orchestration solutions (Atos YSTIA Suite, with Alien4Cloud and Yorc, based on TOSCA), the High-End Application Execution Middleware HEAppE, and new hardware capabilities for maximising efficiency in data processing, analysis and transfer (e.g. Burst Buffers with GPU- and FPGA-based data reprocessing). LEXIS handles computation tasks and data from three Pilots, based on representative and demanding HPC/Cloud-Computing use cases in Industry (SMEs) and Science: i) Simulations of complex turbomachinery and gearbox systems in Aeronautics, ii) Tsunami simulations and earthquake loss assessments which are time-constrained to enable immediate warnings and to support well-informed decisions, and iii) Weather and Climate simulations where massive amounts of in-situ data are assimilated to improve forecasts. A user-friendly LEXIS web portal, as a unique entry point, will provide access to data as well as workflow-handling and remote visualisation functionality. As part of its back-end, LEXIS builds an elaborate system for the handling of input, intermediate and result data. At its core, a Distributed Data Infrastructure (DDI) ensures the availability of LEXIS data at all participating HPC sites, which will be federated with a common LEXIS Authentication and Authorisation Infrastructure (with unified security model, user database and policies). The DDI leverages best of breed data-management solutions from EUDAT, such as B2SAFE (based on iRODS) and B2HANDLE. REST APIs on top of it will ensure a smooth interaction with LEXIS workflows and the orchestration layer. Last, but not least, the DDI will provide functionalities for Research Data Management following the FAIR principles (“Findable, Interoperable, Accessible, Reusable”), e.g. DOI acquisition, which helps to publish and disseminate open data products.}
}
@inproceedings{21:sc:quantum,
  title        = {Towards Optimal Graph Coloring Using Rydberg Atoms},
  author       = {Vitali, Giacomo and Viviani, Paolo and Vercellino, Chiara and Scarabosio, Andrea and Scionti, Alberto and Terzo, Olivier and Giusto, Edoardo and Montrucchio, Bartolomeo},
  year         = 2021,
  booktitle    = {The International Conference for High Performance Computing, Networking, Storage, and Analysis, Research posters},
  location     = {St. Louis, MO, USA},
  publisher    = {},
  address      = {},
  pages        = {},
  doi          = {},
  isbn         = {},
  url          = {https://sc21.supercomputing.org/presentation/?id=rpost113&sess=sess278},
  abstract     = {Quantum mechanics is expected to revolutionize the computing landscape in the near future. Among the many candidate technologies for building universal quantum computers, Rydberg atoms-based systems stand out for being capable of performing both quantum simulations and working as gate-based universal quantum computers while operating at room temperature through an optical system. Moreover, they can potentially scale up to hundreds of quantum bits (qubits). In this work, we solve a Graph Coloring problem by iteratively computing the solutions of Maximal Independent Set (MIS) problems, exploiting the Rydberg blockade phenomenon. Experimental results using a simulation framework on the CINECA Marconi-100 supercomputer demonstrate the validity of the proposed approach.},
  numpages     = {},
  keywords     = {quantum computing,graph coloring,graph,neutral atoms,quantum simulator,hpc}
}
@inproceedings{20:sac:blockchain,
  title        = {Authenticated and Auditable Data Sharing via Smart Contract},
  author       = {Reniers, Vincent and Gao, Yuan and Zhang, Ren and Viviani, Paolo and Madhusudan, Akash and Lagaisse, Bert and Nikova, Svetla and Van Landuyt, Dimitri and Lombardi, Riccardo and Preneel, Bart and Joosen, Wouter},
  year         = 2020,
  booktitle    = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
  location     = {Brno, Czech Republic},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  series       = {SAC '20},
  pages        = {324–331},
  doi          = {10.1145/3341105.3373957},
  isbn         = 9781450368667,
  url          = {https://doi.org/10.1145/3341105.3373957},
  abstract     = {Our main use case features multiple companies that iteratively optimize on the architectural properties of aircraft components in a decentralized manner. In each optimization step of the so-called multi-disciplinary optimization (MDO) process, sensitive data is exchanged between organizations, and we require auditability and traceability of actions taken to assure compliance with signed legal agreements.In this paper, we present a distributed protocol that coordinates authenticated and auditable exchanges of files, leveraging a smart contract. The entire life cycle of a file exchange, including file registration, access request and key distribution, is recorded and traceable via the smart contract. Moreover, when one party raises a dispute, the smart contract can be used to identify the dishonest party without compromising the file's confidentiality.The proposed protocol provides a simple, novel, yet efficient approach to exchange files with support for data access auditability between companies involved in a private consortium with no incentive to share files outside of the protocol. We implemented the protocol in Solidity, deployed it on a private Ethereum blockchain, and validated it within the use case of a decentralized workflow.},
  numpages     = 8,
  keywords     = {blockchain storage, data sharing smart contract, distributed shared ledger, auditable data sharing}
}
@phdthesis{19:dl:viviani:thesis,
  title        = {Deep Learning at Scale with Nearest Neighbours Communications},
  author       = {Paolo Viviani},
  year         = 2019,
  month        = 9,
  doi          = {10.5281/zenodo.3516093},
  url          = {https://zenodo.org/record/3516093/files/20190910_final_pdf.pdf},
  abstract     = {As deep learning techniques become more and more popular, there is the need to move these applications from the data scientist’s Jupyter notebook to efficient and reliable enterprise solutions. Moreover, distributed training of deep learning models will happen more and more outside the well-known borders of cloud and HPC infrastructure and will move to edge and mobile platforms. Current techniques for distributed deep learning have drawbacks in both these scenarios, limiting their long-term applicability. After a critical review of the established techniques for Data Parallel training from both a distributed computing and deep learning perspective, a novel approach based on nearest-neighbour communications is presented in order to overcome some of the issues related to mainstream approaches, such as global communication patterns. Moreover, in order to validate the proposed strategy, the Flexible Asynchronous Scalable Training (FAST) framework is introduced, which allows to apply the nearest-neighbours communications approach to a deep learning framework of choice. Finally, a relevant use-case is deployed on a medium-scale infrastructure to demonstrate both the framework and the methodology presented. Training convergence and scalability results are presented and discussed in comparison to a baseline defined by using state-of-the-art distributed training tools provided by a well-known deep learning framework.},
  keywords     = {fortissimo},
  school       = {Computer Science Department, University of Torino}
}
@inproceedings{19:sac:blockchain,
  title        = {Analysis of Architectural Variants for Auditable Blockchain-based Private Data Sharing},
  author       = {Reniers, Vincent and Van Landuyt, Dimitri and Viviani, Paolo and Lagaisse, Bert and Lombardi, Riccardo and Joosen, Wouter},
  year         = 2019,
  booktitle    = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
  location     = {Limassol, Cyprus},
  publisher    = {ACM},
  address      = {New York, NY, USA},
  series       = {SAC '19},
  pages        = {346--354},
  doi          = {10.1145/3297280.3297316},
  isbn         = {978-1-4503-5933-7},
  url          = {http://doi.acm.org/10.1145/3297280.3297316},
  abstract     = {Many applications by design depend on costly trusted third-party auditors. One such example is the industrial application case of federated multi-disciplinary optimization (MDO), in which different organizations contribute to a complex engineering design effort. Although blockchain and distributed ledger technology (DLT) has strong potential in reducing the dependence on such intermediaries, the architectural complexity involved in designing a solution is daunting. In this paper, we analyze the architectural variants for decentralized private data sharing while guaranteeing auditability in terms of data access operations. Non-repudiation of actions taken by each party is a key requirement, as is availability of the shared data. % through storage governed by the chain. The architectural variants analyzed focus on attaining:~(i)~confidential data exchange, (ii)~maintaining and governing access to the shared data, (iii)~providing data access auditability, (iv)~data validation or conflict resolution, and to a lesser degree (v)~transaction and identity privacy. We systematically enumerate architectural decisions at the levels of:~storage, policy-based file access control, data encryption methods, and auditability mechanisms for private data. This analysis is based on extensive assessment of the state of the art on decentralized private data access management using static or dynamic policies, and private data validation without exposing confidential information. The main contribution of this work is a comprehensive overview of architectural variants for decentralized control of private, encrypted data, and the involved trade-offs in terms of performance, auditable trust and security. These findings are validated in the context on the aforementioned industry case that involves federated multi-disciplinary optimization (MDO).},
  numpages     = 9,
  acmid        = 3297316,
  keywords     = {blockchain storage, decentralized data access control, decentralized private data auditing, distributed shared ledger}
}
@inproceedings{19:gsp:pdp,
  title        = {Accelerating spectral graph analysis through wavefronts of linear algebra operations},
  author       = {Maurizio Drocco and Paolo Viviani and Iacopo Colonnelli and Marco Aldinucci and Marco Grangetto},
  year         = 2019,
  booktitle    = {Proc. of 27th Euromicro Intl. Conference on Parallel Distributed and network-based Processing (PDP)},
  publisher    = {IEEE},
  address      = {Pavia, Italy},
  pages        = {9--16},
  doi          = {10.1109/EMPDP.2019.8671640},
  url          = {https://iris.unito.it/retrieve/handle/2318/1695315/488105/19_wavefront_PDP.pdf},
  abstract     = {The wavefront pattern captures the unfolding of a parallel computation in which data elements are laid out as a logical multidimensional grid and the dependency graph favours a diagonal sweep across the grid. In the emerging area of spectral graph analysis, the computing often consists in a wavefront running over a tiled matrix, involving expensive linear algebra kernels. While these applications might benefit from parallel heterogeneous platforms (multi-core with GPUs),programming wavefront applications directly with high-performance linear algebra libraries yields code that is complex to write and optimize for the specific application. We advocate a methodology based on two abstractions (linear algebra and parallel pattern-based run-time), that allows to develop portable, self-configuring, and easy-to-profile code on hybrid platforms.},
  date-modified = {2019-03-22 23:07:10 +0100},
  keywords     = {eigenvalues, wavefront, GPU, CUDA, linear algebra},
  bdsk-url-1   = {https://iris.unito.it/retrieve/handle/2318/1695315/488105/19_wavefront_PDP.pdf},
  bdsk-url-2   = {https://doi.org/10.1109/EMPDP.2019.8671640}
}
@inproceedings{19:deeplearn:pdp,
  title        = {Deep Learning at Scale},
  author       = {Paolo Viviani and Maurizio Drocco and Daniele Baccega and Iacopo Colonnelli and Marco Aldinucci},
  year         = 2019,
  booktitle    = {Proc. of 27th Euromicro Intl. Conference on Parallel Distributed and network-based Processing (PDP)},
  publisher    = {IEEE},
  address      = {Pavia, Italy},
  pages        = {124--131},
  doi          = {10.1109/EMPDP.2019.8671552},
  url          = {https://iris.unito.it/retrieve/handle/2318/1695211/487778/19_deeplearning_PDP.pdf},
  abstract     = {This work presents a novel approach to distributed training of deep neural networks (DNNs) that aims to overcome the issues related to mainstream approaches to data parallel training. Established techniques for data parallel training are discussed from both a parallel computing and deep learning perspective, then a different approach is presented that is meant to allow DNN training to scale while retaining good convergence properties. Moreover, an experimental implementation is presented as well as some preliminary results.},
  date-modified = {2019-03-22 22:49:35 +0100},
  keywords     = {deep learning, distributed computing, machine learning, large scale, C++},
  bdsk-url-1   = {https://iris.unito.it/retrieve/handle/2318/1695211/487778/19_deeplearning_PDP.pdf}
}
@article{18:deeplearning:arXiv,
  title        = {Pushing the boundaries of parallel Deep Learning - {A} practical approach},
  author       = {Paolo Viviani and Maurizio Drocco and Marco Aldinucci},
  year         = 2018,
  journal      = {CoRR},
  volume       = {abs/1806.09528}
}
@inproceedings{18:hpc4ai_acm_CF,
  title        = {HPC4AI, an AI-on-demand federated platform endeavour},
  author       = {Marco Aldinucci and Sergio Rabellino and Marco Pironti and Filippo Spiga and Paolo Viviani and Maurizio Drocco and Marco Guerzoni and Guido Boella and Marco Mellia and Paolo Margara and Idillio Drago and Roberto Marturano and Guido Marchetto and Elio Piccolo and Stefano Bagnasco and Stefano Lusso and Sara Vallero and Giuseppe Attardi and Alex Barchiesi and Alberto Colla and Fulvio Galeazzi},
  year         = 2018,
  month        = 5,
  booktitle    = {ACM Computing Frontiers},
  address      = {Ischia, Italy},
  doi          = {10.1145/3203217.3205340},
  url          = {http://alpha.di.unito.it/storage/papers/2018_hpc4ai_ACM_CF.pdf},
  abstract     = {In April 2018, under the auspices of the POR-FESR 2014-2020 program of Italian Piedmont Region, the Turin's Centre on High-Performance Computing for Artificial Intelligence (HPC4AI) was funded with a capital investment of 4.5Me and it began its deployment. HPC4AI aims to facilitate scientific research and engineering in the areas of Artificial Intelligence and Big Data Analytics. HPC4AI will specifically focus on methods for the on-demand provisioning of AI and BDA Cloud services to the regional and national industrial community, which includes the large regional ecosystem of Small-Medium Enterprises (SMEs) active in many different sectors such as automotive, aerospace, mechatronics, manufacturing, health and agrifood.},
  date-added   = {2018-04-21 14:18:48 +0000},
  date-modified = {2018-04-21 14:26:05 +0000},
  keywords     = {hpc4ai, c3s}
}
@inproceedings{svd:pdp:18,
  title        = {Scaling Dense Linear Algebra on Multicore and Beyond: a Survey},
  author       = {Paolo Viviani and Maurizio Drocco and Marco Aldinucci},
  year         = 2018,
  booktitle    = {Proc. of 26th Euromicro Intl. Conference on Parallel Distributed and network-based Processing (PDP)},
  publisher    = {IEEE},
  address      = {Cambridge, United Kingdom},
  pages        = {},
  doi          = {},
  url          = {https://iris.unito.it/retrieve/handle/2318/1659340/387685/preprint_aperto.pdf},
  abstract     = {The present trend in big-data analytics is to exploit algorithms with (sub-)linear time complexity, in this sense it is usually worth to  investigate if the available techniques can be approximated to reach an  affordable complexity. However, there are still problems in data science and  engineering that involve algorithms with higher time complexity, like matrix  inversion or Singular Value Decomposition (SVD). This work presents the results of a survey that reviews a number of tools meant  to perform dense linear algebra at ``Big Data'' scale: namely, the proposed  approach aims first to define a feasibility boundary for the problem size of  shared-memory matrix factorizations, then to understand whether it is  convenient to employ specific tools meant to scale out such dense linear algebra  tasks on distributed platforms. The survey will eventually discuss the presented tools from the point of view of  domain experts (data scientist, engineers), hence focusing on the trade-off  between usability and performance.},
  date-modified = {2018-01-30 11:07:31 +0000},
  keywords     = {svd, big data, linear algebra}
}
@inproceedings{18:parco:workflow,
  title        = {Scientific Workflows on Clouds with Heterogeneous and Preemptible Instances},
  author       = {Fabio Tordini and Marco Aldinucci and Paolo Viviani and Ivan Merelli and Pietro Li{\`{o}}},
  year         = 2018,
  booktitle    = {Proc. of the Intl. Conference on Parallel Computing, ParCo 2017, 12-15 September 2017, Bologna, Italy},
  publisher    = {{IOS} Press},
  series       = {Advances in Parallel Computing},
  url          = {https://iris.unito.it/retrieve/handle/2318/1658510/385411/main.pdf},
  abstract     = {The cloud environment is increasingly appealing for the HPC community, which has always dealt with scientific applications. However, there is still some skepticism about moving from traditional physical infrastructures to virtual HPC clusters. This mistrusting probably originates from some well known factors, including the effective economy of using cloud services, data and software availability, and the longstanding matter of data stewardship. In this work we discuss the design of a framework (based on Mesos) aimed at achieving a cost-effective and efficient usage of heterogeneous Processing Elements (PEs) for workflow execution, which supports hybrid cloud bursting over preemptible cloud Virtual Machines.},
  date-added   = {2018-01-21 15:15:01 +0000},
  date-modified = {2018-01-21 15:33:23 +0000}
}
@inbook{17:viviani:advstruct,
  title        = {A Flexible Numerical Framework for Engineering---A Response Surface Modelling Application},
  author       = {Viviani, P. and Aldinucci, M. and d'Ippolito, R. and Lemeire, J. and Vucinic, D.},
  year         = 2018,
  booktitle    = {Improved Performance of Materials: Design and Experimental Approaches},
  publisher    = {Springer International Publishing},
  address      = {Cham},
  pages        = {93--106},
  doi          = {10.1007/978-3-319-59590-0_9},
  isbn         = {978-3-319-59590-0},
  url          = {https://doi.org/10.1007/978-3-319-59590-0_9},
  abstract     = {This work presents an innovative approach adopted for the development of a new numerical software framework for accelerating dense linear algebra calculations and its application within an engineering context. In particular, response surface models (RSM) are a key tool to reduce the computational effort involved in engineering design processes like design optimization. However, RSMs may prove to be too expensive to be computed when the dimensionality of the system and/or the size of the dataset to be synthesized is significantly high or when a large number of different response surfaces has to be calculated in order to improve the overall accuracy (e.g. like when using ensemble modelling techniques). On the other hand, the potential of modern hybrid hardware (e.g. multicore, GPUs) is not exploited by current engineering tools, while they can lead to a significant performance improvement. To fill this gap, a software framework is being developed that enables the hybrid and scalable acceleration of the linear algebra core for engineering applications and especially of RSMs calculations with a user-friendly syntax that allows good portability between different hardware architectures, with no need of specific expertise in parallel programming and accelerator technology. The effectiveness of this framework is shown by comparing an accelerated code to a single-core calculation of a radial basis function RSM on some benchmark datasets. This approach is then validated within a real-life engineering application and the achievements are presented and discussed.},
  date-modified = {2018-03-13 16:40:21 +0000},
  keywords     = {repara, rephrase},
  opteditor    = {{\"O}chsner, Andreas and Altenbach, Holm},
  bdsk-url-1   = {https://doi.org/10.1007/978-3-319-59590-0_9},
  bdsk-url-2   = {http://dx.doi.org/10.1007/978-3-319-59590-0_9}
}
@inproceedings{16:acaces:armadillo,
  title        = {An hybrid linear algebra framework for engineering},
  author       = {Paolo Viviani and Marco Aldinucci and Roberto d'Ippolito},
  year         = 2016,
  month        = {July},
  booktitle    = {Advanced Computer Architecture and Compilation for High-Performance and Embedded Systems (ACACES) -- Poster Abstracts},
  address      = {Fiuggi, Italy},
  url          = {https://iris.unito.it/retrieve/handle/2318/1622382/300198/armadillo.pdf},
  abstract     = {The aim of this work is to provide developers and domain experts with simple (Matlab-like) inter- face for performing linear algebra tasks while retaining state-of-the-art computational speed. To achieve this goal we extend Armadillo C++ library is extended in order to support with multiple LAPACK-compliant back-ends targeting different architectures including CUDA GPUs; moreover our approach involves the possibility of dynamically switching between such back-ends in order to select the one which is most convenient based on the specific problem and hardware configura- tion. This approach is eventually validated within an industrial environment.},
  date-added   = {2016-08-20 17:22:51 +0000},
  date-modified = {2016-08-20 17:29:35 +0000},
  keywords     = {nvidia,algebra, gpu, itea2, repara},
  bdsk-url-1   = {https://iris.unito.it/retrieve/handle/2318/1622382/300198/armadillo.pdf}
}
@inproceedings{17:sac:armadillo,
  title        = {Multiple back-end support for the Armadillo linear algebra interface},
  author       = {Paolo Viviani and Massimo Torquati and Marco Aldinucci and Roberto d'Ippolito},
  year         = 2017,
  month        = 4,
  booktitle    = {In proc. of the 32nd ACM Symposium on Applied Computing (SAC)},
  address      = {Marrakesh, Morocco},
  pages        = {1566--1573},
  url          = {https://iris.unito.it/retrieve/handle/2318/1626229/299089/armadillo_4aperto.pdf},
  abstract     = {The Armadillo C++ library provides programmers with a high-level Matlab-like syntax for linear algebra. Its design aims at providing a good balance between speed and ease of use. It can be linked with different back-ends, i.e. different LAPACK-compliant libraries. In this work we present a novel run-time support of Armadillo, which gracefully extends mainstream implementation to enable back-end switching without recompilation and multiple back-end support. The extension is specifically designed to not affect Armadillo class template prototypes, thus to be easily interoperable with future evolutions of the Armadillo library itself. The proposed software stack is then tested for functionality and performance against a kernel code extracted from an industrial application.},
  date-added   = {2016-08-19 21:47:45 +0000},
  date-modified = {2017-06-13 15:54:43 +0000},
  keywords     = {nvidia, repara, rephrase, itea2},
  bdsk-url-1   = {https://iris.unito.it/retrieve/handle/2318/1626229/299089/armadillo_4aperto.pdf}
}
@inproceedings{16:acex:armadillo,
  title        = {A flexible numerical framework for engineering - a Response Surface Modelling application},
  author       = {Paolo Viviani and Marco Aldinucci and Roberto d'Ippolito and Jean Lemeire and Dean Vucinic},
  year         = 2016,
  booktitle    = {10th Intl. Conference on Advanced Computational Engineering and Experimenting (ACE-X)},
  abstract     = {This work presents the innovative approach adopted for the development of a new numerical software framework for accelerating Dense Linear Algebra calculations and its application within an engineering context. In particular, Response Surface Models (RSM) are a key tool to reduce the computational effort involved in engineering design processes like design optimization. However, RSMs may prove to be too expensive to be computed when the dimensionality of the system and/or the size of the dataset to be synthesized is significantly high or when a large number of different Response Surfaces has to be calculated in order to improve the overall accuracy (e.g. like when using Ensemble Modelling techniques). On the other hand, it is a known challenge that the potential of modern hybrid hardware (e.g. multicore, GPUs) is not exploited by current engineering tools, while they can lead to a significant performance improvement. To fill this gap, a software framework is being developed that enables the hybrid and scalable acceleration of the linear algebra core for engineering applications and especially of RSMs calculations with a user-friendly syntax that allows good portability between different hardware architectures, with no need of specific expertise in parallel programming and accelerator technology. The effectiveness of this framework is shown by comparing an accelerated code to a single-core calculation of a Radial Basis Function RSM on some benchmark datasets. This approach is then validated within a real-life engineering application and the achievements are presented and discussed.},
  date-added   = {2016-08-19 21:37:19 +0000},
  date-modified = {2017-06-19 15:35:39 +0000},
  keywords     = {repara, rephrase, nvidia, gpu}
}
@mastersthesis{tesi:viviani:15,
  title        = {Parallel Computing Techniques for High Energy Physics},
  author       = {Paolo Viviani},
  year         = 2015,
  abstract     = {Modern experimental achievements, with LHC results as a prominent but not exclusive representative, have undisclosed a new range of challenges concerning theoretical com- putations. Tree level QED calculation are no more satisfactory due to the very small experimental uncertainty of precision e+ e- measurements, so Next To Leading and Next to Next to Leading Order calculations are required. At the same time many-legs, high-order QCD processes needed to simulate LHC events are raising even more the bar of computational complexity. The drive for the present work has been the interest in calculating high multiplicity Higgs boson processes with a dedicated software library (RECOLA) currently under development at the University of Torino, as well as the related technological challenges. This thesis undertakes the task of exploring the possibilities offered by present and upcoming computing technologies in order to face these challenges properly. The first two chapters outlines the theoretical context and the available technologies. In chapter 3 a a case study is examined in full detail, in order to explore the suitability of different parallel computing solutions. In the chapter 4, some of those solutions are implemented in the context of the RECOLA library, allowing it to handle processes at a previously unexplored scale of complexity. Alongside, the potential of new, cost-effective parallel architectures is tested.},
  date-added   = {2015-09-27 12:36:54 +0000},
  date-modified = {2015-09-27 13:28:24 +0000},
  keywords     = {fastflow,impact},
  school       = {Physics Department, University of Torino}
}
